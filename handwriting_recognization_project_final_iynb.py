# -*- coding: utf-8 -*-
"""Handwriting recognization  Project Final.iynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZSt32ICS0rB073_jlwz52XVhE0EoHhDr
"""

import os
os.listdir('/content')

import cv2

image_path = '/content/minist.png'  # Correct path
image = cv2.imread(image_path)

if image is None:
    print("Error: Image could not be loaded.")
else:
    print("Image loaded successfully.")

# preprocessing
import cv2
import os

# Load the grid image
image_path = '/content/minist.png'  # Correct path in Colab
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# Check if the image is loaded successfully
if image is None:
    print("Error: Image could not be loaded.")
else:
    print("Image loaded successfully.")

# Define the grid dimensions
rows, cols = 4, 5  # 4 rows and 5 columns in the grid
h, w = image.shape[0] // rows, image.shape[1] // cols  # Height and width of each cell

# Define the labels based on your grid layout
labels = [
    [5, 0, 4, 1, 9],
    [2, 1, 3, 1, 4],
    [3, 5, 3, 6, 1],
    [7, 2, 8, 6, 9]
]

# Create a directory to save the cropped images
output_dir = './cropped_images'  # Output directory in Colab
os.makedirs(output_dir, exist_ok=True)

# Loop through each cell in the grid and crop it
image_count = 1
for row in range(rows):
    for col in range(cols):
        # Calculate the bounding box of each cell
        x_start, y_start = col * w, row * h
        x_end, y_end = (col + 1) * w, (row + 1) * h

        # Crop the cell
        cropped = image[y_start:y_end, x_start:x_end]

        # Get the label for the current cell
        label = labels[row][col]

        # Save the cropped image with the label in the filename
        filename = f'img{image_count}_label{label}.png'
        cv2.imwrite(os.path.join(output_dir, filename), cropped)
        image_count += 1

print(f"All images have been cropped and saved in {output_dir}")

import shutil
shutil.make_archive('/content/cropped_images', 'zip', './cropped_images')
from google.colab import files
files.download('/content/cropped_images.zip')

# preprocessing , labeling , loading (resize and normalize) ,Loop through the cropped images and load them
for filename in os.listdir(cropped_images_dir):
    if filename.endswith('.png'):
        print(f"Processing file: {filename}")  # Debug: print the filename

        # Get the full path to the image
        image_path = os.path.join(cropped_images_dir, filename)

        # Read the image in grayscale
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

        if image is None:
            continue  # Skip the file if it cannot be loaded

        # Resize the image to the target size
        resized_image = cv2.resize(image, image_size)

        # Normalize the image (scale pixel values to [0, 1])
        resized_image = resized_image / 255.0

        # Extract the label from the filename (e.g., "img10_label4.png")
        try:
            # Extract label by splitting the part after 'label' and converting to integer
            label = int(filename.split('_')[1].replace('label', '').split('.')[0])  # Remove 'label' and convert to int
        except IndexError:
            print(f"Filename format error for {filename}")
            continue  # Skip if there's an issue with filename format
        except ValueError:
            print(f"Error converting label for {filename}")
            continue  # Skip if label cannot be converted to integer

        # Append the resized image and its label
        images.append(resized_image)
        labels.append(label)

# Convert images and labels into numpy arrays
images = np.array(images)
labels = np.array(labels)

# Check the shape of the dataset
print(f"Preprocessed dataset shape: {images.shape}, Labels shape: {labels.shape}")

from sklearn.model_selection import train_test_split



# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Print the shapes of the train and test datasets
print(f"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}")
print(f"Training labels shape: {y_train.shape}, Testing labels shape: {y_test.shape}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Reshape images to add a channel dimension (grayscale images have 1 channel)
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)

# Normalize the image data to range [0, 1]
X_train = X_train / 255.0
X_test = X_test / 255.0

# Convert labels to one-hot encoding
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Build the CNN model
model = Sequential()

# First convolutional layer
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Second convolutional layer
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Flatten the output from the convolutional layers
model.add(Flatten())

# Dense layer for classification
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))  # To prevent overfitting

# Output layer with 10 units (one for each digit 0-9)
model.add(Dense(10, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

from tensorflow.keras.utils import to_categorical
import numpy as np

# Assuming y_train contains labels as integers (0 to 9), for example:
y_train = np.array([5, 0, 4, 1, 9])  # Example labels for 5 samples

# Check the shape of y_train before one-hot encoding
print("Before one-hot encoding:", y_train.shape)

# One-hot encode the labels
y_train_encoded = to_categorical(y_train, num_classes=10)

# Check the shape of y_train after one-hot encoding
print("After one-hot encoding:", y_train_encoded.shape)

# Print the one-hot encoded labels for verification
print("One-hot encoded labels:\n", y_train_encoded)

from tensorflow.keras.utils import to_categorical

# Assuming y_train and y_val contain integer labels (e.g., 0-9 for MNIST)
y_train = to_categorical(y_train, num_classes=10)  # Convert to one-hot encoding
y_val = to_categorical(y_val, num_classes=10)  # Convert to one-hot encoding

print("Training data shape:", X_train.shape)
print("Validation data shape:", X_val.shape)

print("Training labels shape:", y_train.shape)
print("Validation labels shape:", y_val.shape)

print("Model output shape:", model.output_shape)

import numpy as np
import os

# Initialize lists to store images and labels
images = []
labels = []

# Example filenames
file_names = [
    "img9_label1.png", "img8_label3.png", "img7_label1.png", "img6_label2.png",
    "img5_label9.png", "img4_label1.png", "img3_label4.png", "img2_label0.png",
    "img20_label9.png", "img1_label5.png", "img19_label6.png", "img18_label8.png",
    "img17_label2.png", "img16_label7.png", "img15_label1.png", "img14_label6.png",
    "img13_label3.png", "img12_label5.png", "img11_label3.png", "img10_label4.png"
]

for filename in file_names:
    # Extract label from filename (e.g., "img9_label1.png" -> 1)
    label = int(filename.split('_')[1].split('.')[0].replace('label', ''))

    # Load image (replace with actual image loading code)
    image = np.random.random((28, 28))  # Placeholder image, replace with actual loading logic

    images.append(image)
    labels.append(label)

# Convert to numpy arrays
X = np.array(images)
y = np.array(labels)

# Check the shapes
print("Original images shape:", X.shape)
print("Original labels shape:", y.shape)

# One-hot encode the labels (assuming labels range from 0 to 9)
from tensorflow.keras.utils import to_categorical
y = to_categorical(y, num_classes=10)

# Check the one-hot encoded labels shape
print("One-hot encoded labels shape:", y.shape)

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Check dataset shape
print(x_train.shape, y_train.shape)  # Should output: (60000, 28, 28) (60000,)

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values to range [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Reshape for CNN input (28x28x1 for grayscale images)
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Custom labels (map MNIST labels to your custom labels)
custom_labels = [5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9]

# Map MNIST labels to your custom labels
y_train = np.array([custom_labels[label] for label in y_train])
y_test = np.array([custom_labels[label] for label in y_test])

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

print("Original labels in MNIST:", np.unique(y_train))
print("Mapped labels:", np.unique([custom_labels[label] for label in range(10)]))

custom_labels = [5, 0, 4, 1, 9, 2, 6, 3, 7, 8]  # Unique mapping

import collections

# Count frequency of each mapped label
label_counts_train = collections.Counter(y_train)
label_counts_test = collections.Counter(y_test)

print("Frequency of mapped labels (y_train):", label_counts_train)
print("Frequency of mapped labels (y_test):", label_counts_test)

# Ensure every original label maps to a unique value from 0-9
custom_labels = [5, 0, 4, 1, 9, 2, 6, 3, 7, 8]

print("Original shape of y_train:", y_train.shape)
print("Original shape of y_test:", y_test.shape)

from tensorflow.keras.datasets import mnist
import numpy as np

# Reload MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

print("Original labels in MNIST (y_train):", np.unique(y_train))
print("Original labels in MNIST (y_test):", np.unique(y_test))

custom_labels = [5, 0, 4, 1, 9, 2, 6, 3, 7, 8]

import numpy as np
from collections import Counter

# Map the original labels to custom labels
y_train_mapped = np.array([custom_labels[label] for label in y_train])
y_test_mapped = np.array([custom_labels[label] for label in y_test])

# Check unique values and their frequencies
print("Mapped labels (y_train):", np.unique(y_train_mapped))
print("Mapped labels (y_test):", np.unique(y_test_mapped))
print("Frequency of mapped labels (y_train):", Counter(y_train_mapped))
print("Frequency of mapped labels (y_test):", Counter(y_test_mapped))

import os
print("Current directory:", os.getcwd())

import os

print("Files and directories in /content:")
print(os.listdir("/content"))

import os

# Search for cropped_images in all directories
for root, dirs, files in os.walk('/content'):
    if 'cropped_images' in dirs:
        print("Found 'cropped_images' at:", os.path.join(root, 'cropped_images'))

from tensorflow.keras.datasets import mnist

# Load dataset (x contains images, y contains labels)
(x_train, y_train), (x_test, y_test) = mnist.load_data()

print(y_test[0])  # This will output the first label in the test set

history = model.fit(x_train, y_train_custom, epochs=5, validation_data=(x_test, y_test_custom))

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Step 1: Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Step 2: Define custom labels (remapped)
custom_labels = [5, 0, 4, 1, 9, 2, 6, 3, 7, 8]

# Step 3: Apply custom mapping
y_train_custom = np.array([custom_labels[label] for label in y_train])
y_test_custom = np.array([custom_labels[label] for label in y_test])

# Step 4: Normalize and reshape
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Reshape to (num_samples, 28, 28, 1) for CNN input
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Step 5: One-hot encode labels
y_train_custom = to_categorical(y_train_custom, 10)
y_test_custom = to_categorical(y_test_custom, 10)

# Step 6: Build CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.25),
    Dense(10, activation='softmax')
])

# Step 7: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 8: Train the model
model.fit(x_train, y_train_custom, epochs=5, batch_size=128, validation_data=(x_test, y_test_custom))

# Step 9: Predict on test set
predictions = model.predict(x_test)

# Step 10: Reverse custom label mapping for display
reverse_mapping = {v: k for k, v in enumerate(custom_labels)}

# Step 11: Visualize 20 predictions
plt.figure(figsize=(12, 10))
for i in range(20):
    plt.subplot(4, 5, i + 1)
    image = x_test[i].reshape(28, 28)
    plt.imshow(image, cmap='gray')

    true_label = reverse_mapping[np.argmax(y_test_custom[i])]
    pred_label = reverse_mapping[np.argmax(predictions[i])]

    color = 'green' if true_label == pred_label else 'red'
    plt.title(f"True: {true_label}, Pred: {pred_label}", color=color)
    plt.axis('off')

plt.tight_layout()
plt.show()

history = model.fit(x_train, y_train_custom, epochs=5, validation_data=(x_test, y_test_custom))

import matplotlib.pyplot as plt

# Convert accuracy to percentage
train_acc = [round(acc * 100, 2) for acc in history.history['accuracy']]
val_acc = [round(acc * 100, 2) for acc in history.history['val_accuracy']]
loss = history.history['loss']
val_loss = history.history['val_loss']

# Create the plot
plt.figure(figsize=(12, 5))

# ----- Accuracy Plot -----
plt.subplot(1, 2, 1)
plt.plot(train_acc, label='Train Accuracy', marker='o', color='blue')
plt.plot(val_acc, label='Validation Accuracy', marker='o', color='orange')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.xticks(range(len(train_acc)))
plt.legend()

# Add accuracy percentage values (Train above, Val below)
for i in range(len(train_acc)):
    plt.text(i, train_acc[i] + 0.1, f"{train_acc[i]}%", color='blue', ha='center', fontsize=9)
    plt.text(i, val_acc[i] - 0.4, f"{val_acc[i]}%", color='orange', ha='center', fontsize=9)

# ----- Loss Plot -----
plt.subplot(1, 2, 2)
plt.plot(loss, label='Train Loss', marker='o', color='blue')
plt.plot(val_loss, label='Validation Loss', marker='o', color='orange')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.xticks(range(len(loss)))
plt.legend()

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Step 1: Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Step 2: Normalize and reshape
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

# Step 3: One-hot encode labels (no remapping)
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Step 4: Build CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.25),
    Dense(10, activation='softmax')
])

# Step 5: Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Step 6: Train the model
model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test))

# Step 7: Predict on test set
predictions = model.predict(x_test)

# Step 8: Visualize 20 predictions
plt.figure(figsize=(12, 10))
for i in range(20):
    plt.subplot(4, 5, i + 1)
    image = x_test[i].reshape(28, 28)
    plt.imshow(image, cmap='gray')

    true_label = np.argmax(y_test[i])
    pred_label = np.argmax(predictions[i])

    color = 'green' if true_label == pred_label else 'red'
    plt.title(f"True: {true_label}, Pred: {pred_label}", color=color)
    plt.axis('off')

plt.tight_layout()
plt.show()